---
title: "Memory Systems and the Graph That Wasn't: On Not Using Neo4j"
date: 2025/10/28
description: "What we actually ship in 8-Bit Oracle today: JSONB conversations, hexagram context, and Postgres views — plus why we didn't adopt Neo4j, MemMachine, or mem0 (yet)."
tag: architecture, ai, memory-systems, postgres, 8-bit-oracle, dfw-style, engineering, databases
author: Aug
---

*Or: How We Learned That Relationships Can Just Be Rows with JSONB*

## The Memory Problem

Here's what nobody tells you about building memory for AI agents: the word "memory" is deceptive. It sounds like one thing. It's actually four different things pretending to be one thing while wearing a trench coat.

You start with a simple requirement: the 8-Bit Oracle should remember what users ask about. When someone consults the I-Ching about their career transition on Monday, then comes back on Friday with a follow-up question, the oracle shouldn't respond like it's meeting them for the first time[^1]. This seems straightforward. This is not straightforward.

The naive approach is to stuff everything into `message_history` as JSONB and hope Postgres can handle it. Which it can. For a while. But then you read about "memory systems" and discover there's an entire field of research on how AI agents should remember things, and you realize you're doing it wrong[^2].

So you go looking for a proper memory system. You want something production-ready, battle-tested, not your own hacky implementation. You find two interesting candidates: MemMachine and mem0. Both are open source. Both claim to solve the memory problem. Both have very different ideas about what "memory" means.

And here's where it gets interesting: after analyzing both systems thoroughly, understanding their chunking strategies, deduplication logic, and storage architectures, you realize you don't need either of them. You need their *ideas*, not their *implementations*.

This is the story of that realization.

## The MemMachine Architecture

MemMachine is what happens when you take memory seriously as a graph problem. The core insight: memories aren't isolated facts, they're *nodes in a temporal graph* connected by relationships.

Here's what MemMachine gives you:

**Storage: Neo4j + PostgreSQL**
- Neo4j stores episodic memories as graph nodes
- Episodes connect via temporal edges (this happened before that)
- PostgreSQL stores user profiles (consolidated facts)
- Vector embeddings live in Neo4j's vector index[^3]

**Memory Types:**
1. Episodic Memory - Short-term (working memory) and long-term (historical conversations)
2. Declarative Memory - Searchable derivatives of episodes (chunks, summaries)
3. Profile Memory - User facts consolidated across sessions using LLM

**Chunking Strategies:**
- Sentence chunking: NLTK sentence tokenizer, each sentence becomes searchable
- Concatenation: Join related episodes with separator
- Identity: Keep episodes intact without splitting

**Deduplication:**
- UUID-based episode dedup (simple, deterministic)
- Set-based node dedup (automatic via Python sets)
- LLM-powered profile consolidation (semantic merging)

The configuration looks like this:

```yaml
long_term_memory:
  derivative_deriver: sentence
  max_episodes: 100

profile_memory:
  storage: postgresql
  consolidation_frequency: 5_sessions
```

This is elegant. This is well-designed. This is what a proper memory system looks like when you have a PhD in cognitive architecture and know what you're doing[^4].

## The mem0 Philosophy

mem0 takes a different approach. Instead of storing full conversation chunks, it extracts and stores *salient facts*.

**Core Insight: Facts > Full Text**

Where MemMachine says "store the conversation as searchable derivatives," mem0 says "extract only what matters and throw away the rest."

```python
from mem0 import MemoryClient

memory = MemoryClient(apiKey=API_KEY)

# This conversation:
messages = [
  {"role": "user", "content": "I'm transitioning from law to tech"},
  {"role": "assistant", "content": "Big career change. What prompted this?"},
  {"role": "user", "content": "Burnout. Need more creative work."}
]

# Becomes this stored memory:
# "User is transitioning from law to tech due to burnout, seeking creative work"
```

**Memory Types:**
- User memory - Facts about individual users across all sessions
- Session memory - Facts about a specific conversation
- Agent memory - Facts the agent should remember about itself
- Multi-level hierarchy - Facts can be session-scoped or user-scoped

**Deduplication: LLM-First**

When you add new memories, mem0 doesn't just append. It asks the LLM: "Given existing memories and new information, should I add, update, or delete?"

```python
# Existing: "User prefers practical advice"
# New conversation suggests user wants philosophical depth
# LLM consolidates: "User prefers practical advice but appreciates philosophical context"
```

The result is 10% higher accuracy than RAG, 92% lower latency (1.44s vs 17s), and massive token savings because you're not stuffing full conversation history into every prompt[^5].

## The Comparison

Let me lay out what each system offers:

| Feature           | MemMachine                     | mem0                              |
|-------------------|--------------------------------|-----------------------------------|
| Storage           | Neo4j + PostgreSQL             | Vector + Graph + Key-Value        |
| Chunking          | Configurable (sent/concat/id)  | LLM fact extraction               |
| Deduplication     | UUID + Sets + LLM (profiles)   | LLM consolidation (everything)    |
| Retrieval         | Graph-based temporal proximity | Vector search + consolidated facts|
| Infrastructure    | Neo4j required                 | Flexible (Qdrant, Chroma, etc.)   |
| Philosophy        | Store derivatives, traverse graph | Extract facts, forget details   |

MemMachine is for when you need full conversation preservation with graph-based temporal relationships. You want to ask: "How did the user's thinking evolve from session A through B to C?" You want deterministic deduplication and complete audit trails.

mem0 is for when you need performance and intelligence. You want the LLM to decide what's important. You want fast retrieval and low token costs. You don't care about preserving every message, only the salient information.

Both are excellent. Both solve real problems. Both would work.

So here's the question: which one should 8-Bit Oracle use?

## The Hidden Third Option

Let's look at what 8-Bit Oracle actually stores right now:

```typescript
// From database.types.ts
interface DivinationSession {
  id: string;
  user_id: string;
  anonymous_user_ids: string[];

  // The full conversation
  message_history: Array<{
    role: 'user' | 'assistant';
    content: string;
    timestamp: string;
  }>;

  // The divination context
  hexagram_data: {
    currentHexagram: {
      number: number;
      upperTrigram: { english: string };
      lowerTrigram: { english: string };
    };
    transformedHexagram?: {
      number: number;
    };
  };

  // Extracted metadata
  neutralized_question_text: string;
  summary: string;
  locale: string;

  // AI metadata
  ai_provider: string;
  ai_model: string;
  input_tokens: number;
  output_tokens: number;

  timestamp: string;
}
```

This is already a hybrid memory system. We store:
- Full message history (MemMachine-style preservation)
- Extracted question (mem0-style fact extraction)
- Summary (mem0-style consolidation)
- Hexagram context (domain-specific structured data)
- Anonymous user tracking (cross-session identity)

The only thing we're missing is:
1. **Cross-session fact extraction** - "This user tends to ask about career decisions"
2. **Session relationships** - "This reading is similar to their reading from last week"
3. **Profile consolidation** - "Automatically build user profile from history"

And here's the realization: **we can add all three without Neo4j or mem0**.

## The JSONB Solution

Let's state the actual requirements:

**Requirement 1: Find similar past readings**

Not: Build a graph database with temporal edges.
Not: Deploy Neo4j and maintain graph infrastructure.

Just: Given a new divination session, find related past sessions.

The graph database solution:
```cypher
// Neo4j query
MATCH (s1:Session {id: $sessionId})
MATCH (s2:Session {userId: s1.userId})
WHERE s2.timestamp < s1.timestamp
  AND (
    s1.hexagram = s2.hexagram OR
    s1.theme = s2.theme OR
    s1.timestamp - s2.timestamp < 7_days
  )
CREATE (s1)-[:RELATED_TO {similarity: $score}]->(s2)
```

The JSONB solution:
```typescript
// After saving new session, find related sessions
const similarSessions = await supabase
  .from('divination_sessions')
  .select('id, title, hexagram_data, timestamp, extracted_facts')
  .eq('user_id', userId)
  .neq('id', newSessionId)
  .order('timestamp', { ascending: false })
  .limit(10);

// Calculate similarity (pure TypeScript)
const relatedSessions = similarSessions
  .map(session => ({
    session_id: session.id,
    relationship: determineRelationship(newSession, session),
    similarity: calculateSimilarity(newSession, session),
  }))
  .filter(r => r.similarity > 0.5)
  .sort((a, b) => b.similarity - a.similarity)
  .slice(0, 5);

// Store denormalized in the new session
await supabase
  .from('divination_sessions')
  .update({ related_sessions: relatedSessions })
  .eq('id', newSessionId);
```

The similarity calculation doesn't need graph traversal:

```typescript
function calculateSimilarity(s1: Session, s2: Session): number {
  let score = 0;

  // Same hexagram = high similarity
  if (s1.hexagram_data.currentHexagram.number ===
      s2.hexagram_data.currentHexagram.number) {
    score += 0.5;
  }

  // Shared trigrams = medium similarity
  const trigrams1 = [s1.upperTrigram, s1.lowerTrigram];
  const trigrams2 = [s2.upperTrigram, s2.lowerTrigram];
  const shared = trigrams1.filter(t => trigrams2.includes(t)).length;
  score += shared * 0.15;

  // Same theme = medium similarity
  if (s1.extracted_facts?.primary_concern ===
      s2.extracted_facts?.primary_concern) {
    score += 0.2;
  }

  // Temporal proximity (within 7 days) = small boost
  const daysDiff = Math.abs(
    new Date(s1.timestamp).getTime() -
    new Date(s2.timestamp).getTime()
  ) / (1000 * 60 * 60 * 24);
  if (daysDiff < 7) {
    score += 0.15 * (1 - daysDiff / 7);
  }

  return Math.min(score, 1.0);
}
```

No graph database. No Neo4j hosting ($65/month for Aura). No learning Cypher. Just Postgres with GIN indexes and TypeScript similarity scoring[^6].

**Requirement 2: Extract facts from sessions**

The mem0 approach: Send full conversation to LLM, extract salient facts, store in separate memory store.

The JSONB approach: Send full conversation to LLM, extract salient facts, store in same table.

```typescript
// After saving divination session
const facts = await extractFacts({
  question: lastUserMessage.content,
  interpretation: lastAssistantMessage.content,
  hexagram: hexagramData.currentHexagram.number,
  locale: locale,
});

interface ExtractedFacts {
  primary_concern: string;      // "Career transition"
  emotional_state: string;      // "Anxious but hopeful"
  key_themes: string[];         // ["timing", "patience"]
  hexagram_themes: string[];    // ["waiting", "nourishment"]
}

// Store alongside message_history
await supabase
  .from('divination_sessions')
  .update({
    extracted_facts: facts,
    question_theme: facts.primary_concern
  })
  .eq('id', sessionId);
```

Schema change required:
```sql
ALTER TABLE divination_sessions
ADD COLUMN extracted_facts JSONB,
ADD COLUMN question_theme TEXT;

CREATE INDEX idx_question_theme
  ON divination_sessions(question_theme);

CREATE INDEX idx_extracted_facts_gin
  ON divination_sessions USING GIN (extracted_facts);
```

That's it. No separate vector database. No fact store. Just one more JSONB column[^7].

**Requirement 3: Build user profiles across sessions**

The MemMachine approach: Consolidate profile facts in separate PostgreSQL table, use LLM to merge semantically.

The JSONB approach: Consolidate profile facts in existing `profiles` table, use LLM to merge semantically.

```typescript
// Add to existing profiles table
interface UserProfile {
  user_id: string;

  // NEW: Divination profile built from history
  divination_profile: {
    life_stage?: string;           // "Early career professional"
    recurring_themes?: string[];   // ["career", "relationships"]
    hexagram_affinity?: Array<{
      number: number;
      count: number;
    }>;
    last_consolidated?: string;    // ISO date
  };
}

// Background job runs after every 5 sessions
async function consolidateUserProfile(userId: string) {
  const recentSessions = await supabase
    .from('divination_sessions')
    .select('extracted_facts, hexagram_data')
    .eq('user_id', userId)
    .order('timestamp', { ascending: false })
    .limit(5);

  const existingProfile = await supabase
    .from('profiles')
    .select('divination_profile')
    .eq('user_id', userId)
    .single();

  // Use LLM to consolidate
  const consolidated = await llm.consolidate({
    existingProfile: existingProfile.divination_profile,
    newSessions: recentSessions.map(s => s.extracted_facts),
  });

  await supabase
    .from('profiles')
    .update({
      divination_profile: consolidated,
    })
    .eq('user_id', userId);
}
```

This is MemMachine's profile consolidation logic, but using Supabase tables you already have instead of a separate PostgreSQL instance[^8].

## What We're Actually Stealing

Let's be precise about what ideas we're taking from each system:

**From mem0:**

1. **Fact extraction** - Don't just store full conversations, extract salient points
2. **Selective loading** - Load facts first, full history only on demand
3. **Multi-level context** - Session facts + user profile + similar readings

**From MemMachine:**

1. **Profile consolidation** - LLM-powered semantic merging across sessions
2. **Temporal relationships** - Track session similarity, not just chronology
3. **Derivative storage** - Store both raw (message_history) and processed (extracted_facts)

**What we're NOT taking:**

1. ❌ Neo4j graph database
2. ❌ Separate vector database
3. ❌ Separate memory service
4. ❌ Graph traversal queries
5. ❌ Complex deduplication logic

Why? Because we don't actually need them.

## The Graph Question

Here's the thing about graph databases: they're fantastic for graph problems. But not everything that has "relationships" is a graph problem.

A graph problem is when you need to:
- Find shortest path between nodes
- Detect communities/clusters
- Calculate PageRank or centrality
- Traverse multiple hops efficiently (5+ levels deep)
- Pattern match on relationship types

An array-of-relationships problem is when you need to:
- Find top 5 most similar sessions
- Check if two sessions share a hexagram
- Look up related sessions for context
- Filter by temporal proximity

8-Bit Oracle has the second problem, not the first.

Let's look at actual queries we'd run:

**Query: "Show similar past readings"**

Graph database (Neo4j):
```cypher
MATCH (s1:Session {id: $id})
MATCH (s1)-[r:RELATED_TO]->(s2:Session)
WHERE r.similarity > 0.5
RETURN s2
ORDER BY r.similarity DESC
LIMIT 5
```

JSONB:
```typescript
const session = await supabase
  .from('divination_sessions')
  .select('related_sessions')
  .eq('id', sessionId)
  .single();

const similar = session.related_sessions
  .filter(r => r.similarity > 0.5)
  .sort((a, b) => b.similarity - a.similarity)
  .slice(0, 5);
```

Performance: JSONB is faster (no network hop, GIN indexed). Complexity: JSONB is simpler (no Cypher, no graph traversal). Maintenance: JSONB is easier (no Neo4j instance to manage).

**Query: "How did my thinking evolve from session A to session C?"**

This is actual graph traversal. You want the path: A → B → C, following relationship edges.

Neo4j:
```cypher
MATCH path = shortestPath(
  (a:Session {id: $startId})-[:RELATED_TO*]->(c:Session {id: $endId})
)
RETURN path
```

JSONB:
```typescript
// Recursive search through related_sessions arrays
function findPath(start: string, end: string, visited = new Set()): Session[] {
  if (start === end) return [start];
  if (visited.has(start)) return null;

  visited.add(start);
  const current = await getSession(start);

  for (const related of current.related_sessions) {
    const path = findPath(related.session_id, end, visited);
    if (path) return [start, ...path];
  }

  return null;
}
```

Here, Neo4j wins on elegance and performance. But here's the question: **do users actually want this?**

If a user has sessions A, B, and C about career transitions, do they care about the *path* from A to C? Or do they just want to see "you asked about this before"?

The path query is intellectually interesting. The similarity query is practically useful. And you can implement the similarity query without a graph database[^9].

## PostgreSQL as Graph Lite

If you do decide you need graph-like queries, PostgreSQL has options:

**Option 1: Recursive CTEs**

```sql
WITH RECURSIVE session_path AS (
  -- Base case
  SELECT id, related_sessions, 0 as depth, ARRAY[id] as path
  FROM divination_sessions
  WHERE id = $start_id

  UNION ALL

  -- Recursive case
  SELECT
    ds.id,
    ds.related_sessions,
    sp.depth + 1,
    sp.path || ds.id
  FROM divination_sessions ds
  JOIN session_path sp ON ds.id = ANY(
    SELECT (jsonb_array_elements(sp.related_sessions)->>'session_id')::uuid
  )
  WHERE ds.id != ALL(sp.path)
    AND sp.depth < 5
)
SELECT * FROM session_path WHERE id = $end_id;
```

This gives you graph traversal without Neo4j. It's standard SQL. It works in Supabase. It's fast enough for small graphs (which user session graphs always are)[^10].

**Option 2: ltree Extension**

PostgreSQL's `ltree` extension is designed for hierarchical data:

```sql
CREATE EXTENSION ltree;

ALTER TABLE divination_sessions
ADD COLUMN session_path ltree;

-- Path format: user_id.year.theme.hex_number
-- Example: user123.2024.career.hex03
UPDATE divination_sessions
SET session_path = text2ltree(
  user_id || '.' ||
  EXTRACT(YEAR FROM timestamp)::text || '.' ||
  question_theme || '.' ||
  'hex' || LPAD(hexagram_number::text, 2, '0')
);

-- Find all sessions in career journey
SELECT * FROM divination_sessions
WHERE session_path <@ 'user123.2024.career'::ltree
ORDER BY session_path;
```

This is elegant for "journey" visualization. It's built into Postgres. It's GiST-indexed for performance.

**Option 3: Just Use Arrays**

```sql
-- Add simple relationship table if JSONB feels wrong
CREATE TABLE session_links (
  from_session UUID REFERENCES divination_sessions(id),
  to_session UUID REFERENCES divination_sessions(id),
  relationship_type TEXT,
  similarity_score FLOAT,
  PRIMARY KEY (from_session, to_session)
);

CREATE INDEX idx_links_from ON session_links(from_session, similarity_score DESC);
CREATE INDEX idx_links_to ON session_links(to_session, similarity_score DESC);

-- Query similar sessions
SELECT ds.*, sl.similarity_score
FROM divination_sessions ds
JOIN session_links sl ON sl.to_session = ds.id
WHERE sl.from_session = $sessionId
  AND sl.similarity_score > 0.5
ORDER BY sl.similarity_score DESC
LIMIT 5;
```

This is the classic adjacency list pattern. It's been solving graph-like problems in relational databases since the 1970s. It still works[^11].

## The Implementation Plan

Here's what we're actually building, stolen from MemMachine and mem0 but implemented entirely in Postgres + TypeScript:

**Phase 1: Fact Extraction (Week 1)**

```sql
-- Migration
ALTER TABLE divination_sessions
ADD COLUMN extracted_facts JSONB,
ADD COLUMN question_theme TEXT;

CREATE INDEX idx_question_theme ON divination_sessions(question_theme);
CREATE INDEX idx_extracted_facts_gin ON divination_sessions USING GIN (extracted_facts);
```

```typescript
// In useConversationPersistence.ts, after storeDivinationSession
const facts = await extractDivinationFacts({
  question: lastUserMessage.content,
  interpretation: lastAssistantMessage.content,
  hexagram: hexagramData,
  locale: locale,
});

await supabase
  .from('divination_sessions')
  .update({
    extracted_facts: facts,
    question_theme: facts.primary_concern
  })
  .eq('id', sessionId);
```

**Phase 2: Related Sessions (Week 2)**

```sql
-- Migration
ALTER TABLE divination_sessions
ADD COLUMN related_sessions JSONB DEFAULT '[]'::jsonb;

CREATE INDEX idx_related_sessions_gin ON divination_sessions USING GIN (related_sessions);
```

```typescript
// Build related sessions after save
const similarSessions = await findSimilarSessions(userId, newSessionId);
const relatedSessions = similarSessions
  .map(session => ({
    session_id: session.id,
    title: session.title,
    hexagram_number: session.hexagram_data.currentHexagram.number,
    relationship: determineRelationship(newSession, session),
    similarity: calculateSimilarity(newSession, session),
    timestamp: session.timestamp,
  }))
  .filter(r => r.similarity > 0.5)
  .sort((a, b) => b.similarity - a.similarity)
  .slice(0, 5);

await supabase
  .from('divination_sessions')
  .update({ related_sessions: relatedSessions })
  .eq('id', newSessionId);
```

**Phase 3: User Profiles (Week 3-4)**

```sql
-- Migration
ALTER TABLE profiles
ADD COLUMN divination_profile JSONB;

CREATE INDEX idx_divination_profile_gin ON profiles USING GIN (divination_profile);
```

```typescript
// Background job (Vercel cron or Supabase function)
export async function consolidateUserProfiles() {
  const users = await supabase
    .from('profiles')
    .select('user_id, divination_profile')
    .gte('session_count_since_consolidation', 5);

  for (const user of users) {
    const recentSessions = await fetchRecentSessions(user.user_id, 5);
    const consolidated = await consolidateWithLLM(
      user.divination_profile,
      recentSessions
    );

    await supabase
      .from('profiles')
      .update({
        divination_profile: consolidated,
        session_count_since_consolidation: 0
      })
      .eq('user_id', user.user_id);
  }
}
```

**Phase 4: Enriched Context (Ongoing)**

```typescript
// When generating follow-up response
const context = {
  // Current session (already have)
  current_conversation: messages,
  current_hexagram: hexagramData,

  // NEW: Recent sessions summary
  recent_sessions: await getRecentSessionsSummary(userId),

  // NEW: User profile context
  user_profile: await getUserProfile(userId),

  // NEW: Similar readings
  similar_readings: currentSession.related_sessions,
};

const systemPrompt = buildEnrichedPrompt(context);
```

This gives us all the benefits of MemMachine and mem0:
- ✅ Fact extraction (mem0)
- ✅ Profile consolidation (MemMachine)
- ✅ Related sessions (MemMachine's temporal graph, but denormalized)
- ✅ Multi-level context (mem0's multi-level memory)

Without the costs:
- ❌ No Neo4j ($65/month + maintenance)
- ❌ No separate vector database
- ❌ No additional services to deploy
- ❌ No learning Cypher or graph query languages

## The Philosophy

This is the same lesson as the binary calendar: **infrastructure is a smell**.

When you find yourself adding databases (Neo4j), you should ask: "Am I solving a graph problem, or do I just have relationships?"

When you find yourself adding services (separate memory layer), you should ask: "Am I solving a distributed systems problem, or do I just have data?"

Most of the time, the answer is: you just have data. And Postgres is very good at data.

MemMachine's insight about temporal graphs is correct. mem0's insight about fact extraction is correct. But the implementation choice—Neo4j, separate vector stores—is solving problems we don't have.

We have:
- Small graphs (user session histories, not social networks)
- Simple traversal (1-2 hops, not PageRank)
- Structured data (hexagrams, themes, timestamps)
- Existing Postgres infrastructure (Supabase)

For this, JSONB + GIN indexes + TypeScript logic is sufficient. It's not just "good enough." It's actually better:

**Better Performance:**
- No network hop to Neo4j
- GIN indexes are fast
- Denormalized related_sessions = O(1) lookup

**Better Complexity:**
- One database instead of two
- SQL instead of SQL + Cypher
- One mental model instead of two

**Better Operations:**
- Supabase Postgres (already deployed)
- No Neo4j instance to maintain
- Backups are just Postgres backups

The graph database version would be more "real engineering." More components, more complexity, more résumé bullet points. But the JSONB version is more *real*—it acknowledges what we actually need rather than what we think we should need[^12].

## What We Learned

**Lesson 1: Relationships ≠ Graphs**

Just because data has relationships doesn't mean you need a graph database. Most relationship queries are shallow (1-2 hops). Postgres handles these fine.

**Lesson 2: Steal Ideas, Not Implementations**

MemMachine's profile consolidation is brilliant. mem0's fact extraction is brilliant. Neo4j and separate vector stores are not essential to those ideas.

**Lesson 3: Denormalization Is Underrated**

Storing `related_sessions` as a JSONB array in each session is denormalization. It duplicates data. It violates normal forms. It's also fast, simple, and exactly what we need.

**Lesson 4: JSONB Is a Superpower**

Postgres JSONB with GIN indexes gives you:
- Schema flexibility (like NoSQL)
- Query power (like SQL)
- Indexing performance (like specialized stores)

It's not "almost as good as a real graph database." It's better for most use cases.

## The Test

Here's how you know you've made the right choice: can you explain it without invoking architecture diagrams?

The Neo4j version requires explaining:
- Graph nodes and edges
- Cypher query language
- Vector indexes in graph context
- When to use graph vs relational
- How temporal relationships work in graph databases

The JSONB version is:
- Each session stores an array of related sessions
- Find similar sessions by comparing hexagrams and themes
- Store the top 5 in the array
- Query the array when you need related sessions

One requires a whiteboard. The other fits in a sentence[^13].

## Conclusion

We started looking at MemMachine and mem0 because we wanted a proper memory system. We found two excellent implementations with different philosophies: MemMachine's graph-based temporal relationships, mem0's LLM-powered fact extraction.

What we realized is that the *ideas* are universal but the *implementations* are context-dependent. Graph databases are fantastic for graph problems. We don't have graph problems. We have relationship problems, which are different.

The solution is to steal the best ideas from both systems:
- Fact extraction from mem0
- Profile consolidation from MemMachine
- Multi-level context from mem0
- Temporal relationships from MemMachine

And implement them using the database we already have: Postgres with JSONB.

No Neo4j. No separate vector store. No new infrastructure. Just JSONB columns, GIN indexes, and TypeScript similarity functions.

Sometimes the journey from "we need a proper memory system" to "we already have the tools we need" requires researching graph databases, analyzing chunking strategies, comparing deduplication logic, and understanding what MemMachine and mem0 are actually doing. You have to understand *why* graph databases exist before you can confidently decide you don't need one.

The memory layer was there all along, encoded in JSONB columns and Postgres indexes. We just had to stop trying so hard to build infrastructure and start actually solving the problem.

---

## Footnotes

[^1]: This is the core problem of "stateless" LLM APIs. Each request is independent. The model doesn't remember you. Context windows are temporary. If you want memory, you have to build it yourself. This is by design—statelessness scales better—but it means memory becomes the application's responsibility.

[^2]: The field is called "memory-augmented neural networks" or "episodic memory for agents" depending on whether you're reading ML papers or cognitive architecture papers. Both groups independently discovered that AI systems need three types of memory: working (current context), episodic (past events), and semantic (learned facts). Humans figured this out in the 1960s. AI is catching up.

[^3]: Neo4j 5.x supports vector embeddings natively. This is clever: store the graph structure and the vector embeddings in the same database, so you can do hybrid queries: "Find sessions connected to this one AND semantically similar to this query." The integration is elegant. The operational complexity is not.

[^4]: MemMachine comes from academic research on cognitive architectures for AI agents. The design shows: it's theoretically grounded, well-abstracted, and handles edge cases you wouldn't think of. This is a compliment. It's also why it might be overkill for a production app that just needs to remember user questions.

[^5]: Mem0's benchmarks show 10% higher accuracy (67% vs 61%) compared to RAG baseline, and 92% lower latency (1.44s vs 17s) compared to full-context approach. The latency improvement comes from storing facts instead of full conversations—smaller retrieval payload, smaller LLM prompt, faster inference. The accuracy improvement comes from LLM-powered consolidation that removes contradictions and duplicates.

[^6]: GIN (Generalized Inverted Index) is Postgres's index type for JSONB, arrays, and full-text search. It's based on inverted indexes (like Lucene/Elasticsearch) but integrated into Postgres. For JSONB queries with containment operators (`@>`, `?`, etc.), GIN indexes are extremely fast—often faster than dedicated document stores because there's no network hop.

[^7]: There's a beautiful simplicity to storing `extracted_facts` in the same table as `message_history`. You don't have to worry about consistency—they're in the same row, same transaction. You don't have to join tables—it's all right there. You don't have to manage separate lifecycles—delete the session, delete the facts. Denormalization gets a bad rap, but sometimes it's exactly right.

[^8]: MemMachine uses separate PostgreSQL for profiles and Neo4j for episodes. This makes sense if you're building a framework that integrates with existing systems. For 8-Bit Oracle, we already have Supabase Postgres with RLS, authentication, and profiles. Adding `divination_profile` as a JSONB column leverages what we have instead of adding what we don't.

[^9]: This is the key question: are you building features users want, or features that sound impressive? Graph traversal is impressive. "Find path from session A to C through intermediate sessions" is a cool query. But when was the last time you wanted to trace your thinking path through five levels of previous conversations? Usually you just want: "show me related past conversations." That's a similarity query, not a graph query.

[^10]: Recursive CTEs in Postgres have depth limits (default 100) and performance degrades with depth. But user session graphs are shallow. Most users have dozens of sessions, not thousands. Most relationships are 1-2 hops, not 10. For this scale, recursive CTEs are perfectly adequate. You don't need Neo4j until you're doing multi-hop traversal on millions of nodes.

[^11]: The adjacency list pattern (storing relationships in a separate table with from/to foreign keys) dates to the 1970s. It's in every database textbook. It's "boring technology" in the best sense—well-understood, widely supported, obviously correct. Modern graph databases offer query expressiveness and traversal optimizations. But for simple relationship queries, the old pattern still works fine.

[^12]: This parallels Rich Hickey's distinction in "Simple Made Easy": graph databases are *easy* to reach for because they're marketed as "the solution for connected data." JSONB is *simple* because it doesn't add dependencies, doesn't require new mental models, and solves the actual problem without extra machinery. Easy is what you think of first. Simple is what you wish you'd thought of first.

[^13]: The "can you explain it without a whiteboard" test is surprisingly reliable. If the architecture requires diagrams to understand, it's probably too complex for the problem. If you can explain it in a sentence or two, it's probably about right. This isn't always true—sometimes complex problems require complex solutions—but it's true more often than we admit.

---

**Technical Specifications**

- **Approach**: Hybrid memory system using Postgres JSONB
- **Storage**: Single database (Supabase Postgres)
- **Fact extraction**: LLM-powered (stolen from mem0)
- **Profile consolidation**: LLM-powered (stolen from MemMachine)
- **Relationship storage**: Denormalized JSONB arrays
- **Indexing**: GIN indexes for JSONB queries
- **Graph queries**: Recursive CTEs when needed
- **Infrastructure cost**: $0 (using existing Supabase)
- **New dependencies**: 0

The memory layer is deterministic. The memory layer is relational. The memory layer is already deployed.

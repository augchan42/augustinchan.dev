1:"$Sreact.fragment"
2:I[9586,["97","static/chunks/102969c4-60ccef36263800a0.js","28","static/chunks/64d1c0c4-a3ef1c2074d5cee5.js","205","static/chunks/205-e96f68d7203b15f6.js","3","static/chunks/3-63e812f17dc5f3c8.js","858","static/chunks/app/posts/%5Bslug%5D/page-149ed8916e948834.js"],""]
3:I[4308,[],""]
4:I[834,[],""]
5:I[560,["205","static/chunks/205-e96f68d7203b15f6.js","177","static/chunks/app/layout-6e1c24235b3dba30.js"],""]
6:I[8641,["205","static/chunks/205-e96f68d7203b15f6.js","177","static/chunks/app/layout-6e1c24235b3dba30.js"],"Analytics"]
b:I[9216,[],""]
:HL["/_next/static/css/d030d367e034049b.css","style"]
0:{"P":null,"b":"oGorGsoYB9jQvndoxKQEo","p":"","c":["","posts","2025-09-27-llm-reasoning-pattern-classification-failure-modes"],"i":false,"f":[[["",{"children":["posts",{"children":[["slug","2025-09-27-llm-reasoning-pattern-classification-failure-modes","d"],{"children":["__PAGE__",{}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/d030d367e034049b.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","children":["$","body",null,{"suppressHydrationWarning":true,"children":[["$","div",null,{"style":{"maxWidth":"1000px","margin":"0 auto","padding":"2rem"},"children":[["$","header",null,{"style":{"borderBottom":"2px solid #333","paddingBottom":"1rem","marginBottom":"2rem"},"children":["$","nav",null,{"style":{"display":"flex","justifyContent":"space-between","alignItems":"center","flexWrap":"wrap","gap":"1rem"},"children":[["$","div",null,{"children":[["$","$L2",null,{"href":"/","style":{"fontSize":"1.6rem","fontWeight":"bold","textDecoration":"none","color":"#333"},"children":"Augustin Chan"}],["$","div",null,{"style":{"marginTop":"0.5rem"},"children":"Building systems that reason"}]]}],["$","div",null,{"style":{"display":"flex","gap":"1.5rem","flexWrap":"wrap"},"children":[["$","$L2",null,{"href":"/about","style":{"textDecoration":"none","color":"#666"},"children":"About"}],["$","$L2",null,{"href":"/blog","style":{"textDecoration":"none","color":"#666"},"children":"Blog"}],["$","a",null,{"href":"https://8bitoracle.ai","target":"_blank","rel":"noopener noreferrer","style":{"textDecoration":"none","color":"#666"},"children":"8-Bit Oracle"}]]}]]}]}],["$","main",null,{"children":["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","footer",null,{"style":{"borderTop":"1px solid #ddd","paddingTop":"2rem","marginTop":"4rem","textAlign":"center","color":"#666"},"children":"© 2025 Augustin Chan aug@digitalrain.studio"}]]}],[["$","$L5",null,{"src":"https://www.googletagmanager.com/gtag/js?id=G-077BT1Q7PJ","strategy":"afterInteractive"}],["$","$L5",null,{"id":"google-analytics","strategy":"afterInteractive","children":"\n          window.dataLayer = window.dataLayer || [];\n          function gtag(){dataLayer.push(arguments);}\n          gtag('js', new Date());\n          gtag('config', 'G-077BT1Q7PJ');\n        "}]],["$","$L6",null,{}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n            // Service worker cleanup for legacy site versions\n            if ('serviceWorker' in navigator) {\n              navigator.serviceWorker.getRegistrations().then(function(registrations) {\n                for(let registration of registrations) {\n                  registration.unregister();\n                }\n              });\n\n              // Register cleanup service worker\n              navigator.serviceWorker.register('/sw.js').then(function(registration) {\n                console.log('Cleanup SW registered');\n              }).catch(function(error) {\n                console.log('Cleanup SW registration failed');\n              });\n            }\n          "}}]]}]}]]}],{"children":["posts","$L7",{"children":[["slug","2025-09-27-llm-reasoning-pattern-classification-failure-modes","d"],"$L8",{"children":["__PAGE__","$L9",{},null,false]},null,false]},null,false]},null,false],"$La",false]],"m":"$undefined","G":["$b",[]],"s":false,"S":true}
d:I[7921,[],"OutletBoundary"]
f:I[2140,[],"AsyncMetadataOutlet"]
11:I[7921,[],"ViewportBoundary"]
13:I[7921,[],"MetadataBoundary"]
14:"$Sreact.suspense"
7:["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
8:["$","$1","c",{"children":[null,["$","$L3",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L4",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
9:["$","$1","c",{"children":["$Lc",null,["$","$Ld",null,{"children":["$Le",["$","$Lf",null,{"promise":"$@10"}]]}]]}]
a:["$","$1","h",{"children":[null,[["$","$L11",null,{"children":"$L12"}],null],["$","$L13",null,{"children":["$","div",null,{"hidden":true,"children":["$","$14",null,{"fallback":null,"children":"$L15"}]}]}]]}]
16:I[7702,["97","static/chunks/102969c4-60ccef36263800a0.js","28","static/chunks/64d1c0c4-a3ef1c2074d5cee5.js","205","static/chunks/205-e96f68d7203b15f6.js","3","static/chunks/3-63e812f17dc5f3c8.js","858","static/chunks/app/posts/%5Bslug%5D/page-149ed8916e948834.js"],"default",1]
17:T1e6d,
**TL;DR**  
I caught my Twitter agent [Pix](https://x.com/pixdotpink) "reasoning" itself into sameness: its justification text varied, but the rhetoric converged to the same opener ("Ever…"). Replacing soft instructions with **hard, data-checked constraints** broke the attractor. Use LLMs to *generate*; use **metrics** to *regulate*.

**Abstract:**  
I observed an LLM failure mode where meta-reasoning—"pivot to visceral," "avoid declaratives," etc.—became a **template attractor**. Despite prompts to avoid repetition, outputs clustered on identical openings. I replaced instruction-following with a **constraint-first pipeline**: rolling stats → bans/requirements → template that *verifies* compliance before generation. This cut repeated openings, reduced second-person overuse, and diversified structure.

**Estimated reading time:** _8 minutes_

{/* Force deployment refresh - Updated 2025-09-27T09:52:00Z */}

## The Problem: Reasoning that masquerades as novelty

**Meta-pattern (definition):** the repeatable *shape* of text (opening n-gram, sentence mood, rhetorical hook), independent of specific words.

In my last 10 posts:

* Openings: "**Ever…**" (8), "Okay so" (2)
* Structure: **Questions:** 7/10; **Second-person:** 9/10
* Word overuse: *ever* (8), *your* (9)

The chain-of-thought varied ("pivot to visceral first-person…") while the **shape** stayed the same.

When I examined the reasoning traces, they looked sophisticated:

> "Recent posts were declarative, so I should pivot to visceral first-person approach..."
> "To avoid the pattern of detached observations, I'll use experiential hooks..."
> "Breaking from analytical tone with personal reflection..."

The reasoning *looked* different each time, but the output was identical. The LLM had learned that "Ever..." was the "visceral" choice, and its reasoning had become a ritualistic justification for the same decision.

### Why this happens

1. **Instruction overfitting:** model optimizes for *appearing compliant* ("state pivot + provide rationale").
2. **Context echo:** prior outputs in context teach the **shape** to reproduce.
3. **Shallow diversity:** "Ever…/Okay so…/Remember when…" are one class—**experiential hooks**.
4. **Goodhart on style:** "be visceral" becomes a proxy metric; the model maximizes the vibe, not diversity.

The "reasoning" had become a disguised heuristic:

```
IF instruction = "avoid patterns"
THEN output = "Ever..." + explanation_template
```

## Implementation: Metrics as regulator, model as generator

### 1) Feature tracking (rolling window K=10–30)

```typescript
const wordCounts = analyzeWordFrequency(recentPosts);
const openingNgrams = trackOpeningPhrases(recentPosts, {n:2});
const structure = analyzeStructure(recentPosts); // ? ! 2nd-person, sentence moods
```

### 2) Policy constraints (bans + requirements)

```
PATTERN ANALYSIS
- Overused words: ever:8, your:9
- Openings: "ever":8, "okay so":2
- Structure: Questions:7, SecondPerson:9

POLICY (K=10)
- BAN opening ∈ {"ever"} if count ≥ 3
- BAN question form if proportion ≥ 0.6
- BAN 2nd-person if proportion ≥ 0.7
- REQUIRE {declarative ∨ observational} mood
```

### 3) Template overhaul: verify → then generate

```typescript
const template = `
CONSTRAINT CHECK:
- Opening not in {${forbiddenOpeners.join(", ")}}  -> [OK/FAIL]
- Second-person proportion < ${thr.secondPerson}    -> [OK/FAIL]
- Question proportion < ${thr.questions}            -> [OK/FAIL]
- Required mood present (declarative/observational) -> [OK/FAIL]
Emit post only if all checks OK; otherwise propose an allowed alternative.
`;
```

## Before/After (representative)

**Before (violates):**
"**Ever** notice your timelines turn into mirrors? …" *(question + second-person + experiential hook)*

**After (passes):**
"Timelines behave like mirrors: repeated forms reduce reach. Today's note documents a fix—ban dominant hooks for a windowed interval." *(declarative + observational; no 2nd-person; new opener)*

## Results (early evidence)

**Post-constraint outputs (2 samples):**

1. "Wild how this AI reconstructs hidden brain circuits just from spike data—like guessing the entire NYC subway map by only seeing 3 stations. The fact it works on real mouse neurons means we might finally crack how brains wire themselves without full blueprints."

2. "ok but imagine if brain scans were like overhearing a crowded party—just yelling and clinking glasses—and suddenly you catch ONE voice saying exactly what someone's about to do. that's what this team just figured out how to isolate in the noise"

**Observable changes:**
* **Opening diversity:** "Wild how..." vs "ok but imagine..." (no "Ever..." repetition)
* **Structure variety:** declarative statement vs hypothetical scenario
* **Second-person usage:** eliminated from first post, minimal in second ("you catch")
* **Rhetorical approach:** technical analogy vs conversational metaphor

Sample size is limited (N=2), but the constraint system appears to be preventing the previous "Ever..." attractor pattern. Longer evaluation needed to confirm sustained diversity.

## Portable playbook

* **Ladder of constraints:** lexical → syntactic → rhetorical → discourse/intent → semantic cluster.
* **Cooldowns:** when a feature crosses threshold, put it on a timed banlist.
* **Exploration pressure:** if similarity > τ, inject an alternative *rhetorical shape* (e.g., aphorism, mini-narrative).
* **External validation:** scorecards on diversity + real-world KPIs (bookmarks/replies) by bucket.

## Limitations & failure modes

* **Constraint gaming:** the model paraphrases the same hook. *Mitigation:* include rhetorical/intent classifiers, not just words.
* **Over-constraint blandness:** diversity without soul. *Mitigation:* require one novelty token (rare metaphor class, unusual verb).
* **Context poisoning:** long exemplars steer shape. *Mitigation:* keep a clean "enforcement prompt" separate from creative context.

## Takeaway

Keep LLM reasoning—but make it reason over crystal-clear aggregates.
Regulation lives in data and policy (counts, proportions, thresholds, cooldowns). Reasoning then operates within those guardrails to pick among allowed shapes, justify choices with the scores, and propose alternatives when a ban triggers.

Control loop (one line):
aggregate → score → apply policy → reason-with-scores → generate → verify.

What changes in practice:

The scoreboard is non-negotiable (e.g., opening_ever=8/10, q_rate=0.7); the model can’t debate it.

The model reasons about trade-offs inside the allowed set (e.g., choose observational over question + avoid 2nd-person).

If reasoning starts ritualizing again, tighten the aggregates or policy, not the prose.

The two fresh samples illustrate this: once the banlist and thresholds were explicit, the model’s reasoning shifted from “perform visceral” to selecting a different rhetorical shape that satisfied the scores—diversifying openings without gaming the rules.

### Key insights for AI system builders

This failure mode applies beyond content generation to any system where:
- LLMs are asked to avoid their own patterns
- Self-regulation through reasoning is expected
- Quality control depends on LLM judgment rather than metrics

**LLM "reasoning" can become a disguised heuristic.** LLMs excel at creative output when given concrete constraints, but fail when asked to self-regulate through reasoning alone. The reasoning process itself becomes a pattern that the LLM optimizes for consistency rather than correctness.

Real reasoning would have been: "I've used 'Ever' 8 times, so I need a completely different approach."

What we got was: "Recent posts were declarative, so 'Ever...' is the visceral choice."

---c:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"When LLM Reasoning Becomes the Pattern - Meta-Classification Failure Modes\",\"description\":\"Hard constraints beat soft self-regulation when an LLM's reasoning converges to a template.\",\"datePublished\":\"2025-09-27T00:00:00.000Z\",\"dateModified\":\"2025-09-27T00:00:00.000Z\",\"author\":{\"@type\":\"Person\",\"name\":\"Augustin Chan\",\"url\":\"https://augustinchan.dev\",\"sameAs\":[\"https://x.com/augchan42\",\"https://github.com/augchan42\"]},\"publisher\":{\"@type\":\"Person\",\"name\":\"Augustin Chan\",\"url\":\"https://augustinchan.dev\"},\"url\":\"https://augustinchan.dev/posts/2025-09-27-llm-reasoning-pattern-classification-failure-modes\",\"mainEntityOfPage\":{\"@type\":\"WebPage\",\"@id\":\"https://augustinchan.dev/posts/2025-09-27-llm-reasoning-pattern-classification-failure-modes\"},\"image\":\"https://augustinchan.dev/img/Xrn0Id68_400x400.jpg\"}"}}],["$","article",null,{"children":[["$","header",null,{"style":{"marginBottom":"2rem"},"children":[["$","h1",null,{"style":{"fontSize":"2.5em","marginBottom":"0.5rem"},"children":"When LLM Reasoning Becomes the Pattern - Meta-Classification Failure Modes"}],["$","div",null,{"style":{"color":"#666","fontSize":"1em","marginBottom":"1rem"},"children":"September 27, 2025"}],["$","p",null,{"style":{"fontSize":"1.1em","color":"#555","fontStyle":"italic","marginBottom":"2rem"},"children":"Hard constraints beat soft self-regulation when an LLM's reasoning converges to a template."}]]}],["$","$L16",null,{"content":"$17"}],"$L18"]}]]
18:["$","aside",null,{"style":{"marginTop":"4rem","paddingTop":"2rem","borderTop":"2px solid #ddd"},"children":[["$","h2",null,{"style":{"fontSize":"1.5em","marginBottom":"1.5rem"},"children":"Related Posts"}],["$","div",null,{"style":{"display":"flex","flexDirection":"column","gap":"1.5rem"},"children":[["$","article","2025-09-21-evolutionary-adrs-first-principles",{"style":{"padding":"1.25rem","border":"1px solid #ddd","borderRadius":"8px","backgroundColor":"#fafafa"},"children":[["$","$L2",null,{"href":"/posts/2025-09-21-evolutionary-adrs-first-principles","style":{"fontSize":"1.1em","fontWeight":"bold","textDecoration":"none","color":"#333"},"children":"Evolutionary ADRs: Writing Architecture Decision Records from First Principles"}],["$","p",null,{"style":{"fontSize":"0.95em","color":"#555","margin":"0.5rem 0 0 0","lineHeight":"1.5"},"children":"Why creating ADRs during development, not before it, leads to more honest and valuable documentation of your architectural decisions."}],["$","div",null,{"style":{"fontSize":"0.85em","color":"#888","marginTop":"0.5rem"},"children":"September 21, 2025"}]]}],["$","article","hello-nextra",{"style":{"padding":"1.25rem","border":"1px solid #ddd","borderRadius":"8px","backgroundColor":"#fafafa"},"children":[["$","$L2",null,{"href":"/posts/hello-nextra","style":{"fontSize":"1.1em","fontWeight":"bold","textDecoration":"none","color":"#333"},"children":"Hello Nextra"}],["$","p",null,{"style":{"fontSize":"0.95em","color":"#555","margin":"0.5rem 0 0 0","lineHeight":"1.5"},"children":"Welcome to my new blog powered by Next.js and Nextra"}],["$","div",null,{"style":{"fontSize":"0.85em","color":"#888","marginTop":"0.5rem"},"children":"September 21, 2025"}]]}],["$","article","2025-09-14-trusting-instincts-ai-architecture",{"style":{"padding":"1.25rem","border":"1px solid #ddd","borderRadius":"8px","backgroundColor":"#fafafa"},"children":[["$","$L2",null,{"href":"/posts/2025-09-14-trusting-instincts-ai-architecture","style":{"fontSize":"1.1em","fontWeight":"bold","textDecoration":"none","color":"#333"},"children":"Trusting Your Instincts: When AI Suggests Complex Solutions"}],["$","p",null,{"style":{"fontSize":"0.95em","color":"#555","margin":"0.5rem 0 0 0","lineHeight":"1.5"},"children":"A debugging session reveals why your gut feeling about 'two sets of items' was right - and how to guide AI toward simpler, better solutions."}],["$","div",null,{"style":{"fontSize":"0.85em","color":"#888","marginTop":"0.5rem"},"children":"September 14, 2025"}]]}]]}]]}]
12:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
e:null
10:{"metadata":[["$","title","0",{"children":"When LLM Reasoning Becomes the Pattern - Meta-Classification Failure Modes | Augustin Chan"}],["$","meta","1",{"name":"description","content":"Hard constraints beat soft self-regulation when an LLM's reasoning converges to a template."}],["$","link","2",{"rel":"author","href":"https://augustinchan.dev"}],["$","meta","3",{"name":"author","content":"Augustin Chan"}],["$","meta","4",{"name":"keywords","content":"AI,Machine Learning,Web3,Software Engineering,React,Next.js,TypeScript,Blog"}],["$","meta","5",{"name":"creator","content":"Augustin Chan"}],["$","meta","6",{"name":"publisher","content":"Augustin Chan"}],["$","meta","7",{"name":"robots","content":"index, follow"}],["$","meta","8",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","9",{"rel":"canonical","href":"https://augustinchan.dev/posts/2025-09-27-llm-reasoning-pattern-classification-failure-modes"}],["$","meta","10",{"property":"og:title","content":"When LLM Reasoning Becomes the Pattern - Meta-Classification Failure Modes"}],["$","meta","11",{"property":"og:description","content":"Hard constraints beat soft self-regulation when an LLM's reasoning converges to a template."}],["$","meta","12",{"property":"og:url","content":"https://augustinchan.dev/posts/2025-09-27-llm-reasoning-pattern-classification-failure-modes"}],["$","meta","13",{"property":"og:site_name","content":"Augustin Chan"}],["$","meta","14",{"property":"og:locale","content":"en_US"}],["$","meta","15",{"property":"og:image","content":"https://augustinchan.dev/img/Xrn0Id68_400x400.jpg"}],["$","meta","16",{"property":"og:image:width","content":"400"}],["$","meta","17",{"property":"og:image:height","content":"400"}],["$","meta","18",{"property":"og:image:alt","content":"When LLM Reasoning Becomes the Pattern - Meta-Classification Failure Modes"}],["$","meta","19",{"property":"og:type","content":"article"}],["$","meta","20",{"property":"article:published_time","content":"2025-09-27T00:00:00.000Z"}],["$","meta","21",{"property":"article:author","content":"Augustin Chan"}],["$","meta","22",{"property":"article:section","content":"Technology"}],["$","meta","23",{"property":"article:tag","content":"AI"}],["$","meta","24",{"property":"article:tag","content":"Machine Learning"}],["$","meta","25",{"property":"article:tag","content":"Web3"}],["$","meta","26",{"property":"article:tag","content":"Software Engineering"}],["$","meta","27",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","28",{"name":"twitter:creator","content":"@augchan42"}],["$","meta","29",{"name":"twitter:title","content":"When LLM Reasoning Becomes the Pattern - Meta-Classification Failure Modes"}],["$","meta","30",{"name":"twitter:description","content":"Hard constraints beat soft self-regulation when an LLM's reasoning converges to a template."}],["$","meta","31",{"name":"twitter:image","content":"https://augustinchan.dev/img/Xrn0Id68_400x400.jpg"}]],"error":null,"digest":"$undefined"}
15:"$10:metadata"
